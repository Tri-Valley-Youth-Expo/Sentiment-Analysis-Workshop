{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Deep Learning Workshop!\n",
    "\n",
    "In this ipython notebook you will learn how to create and train your very own Deep Learning model! \n",
    "# Prerequisites\n",
    "\n",
    "All you need to know is basic python. This notebook will teach you everything else you need to know.\n",
    "\n",
    "# Installation\n",
    "\n",
    "For this demo, you will need Tensorflow 2.0, Pandas, and Matplotlib.\n",
    "\n",
    " 1. Open a terminal and change directories to an empty one\n",
    " 2. Create a python virtual environment by pasting in the following command in the terminal. This makes it easy for all of the packages needed to be contained in your directory. Virtual environments are also very useful for controlling package dependencies for your projects.\n",
    " `python -m venv env`\n",
    " 3. This command creates a virtual environment called env in your current directory. Now, run the following command to activate the virtual environment.\n",
    "**For Mac and Linux**: `source ./env/bin/activate`\n",
    "**For Windows**: `./env/Scripts/activate`\n",
    " 5. To verify that the environment has been activated, look for the `(env)` in the beginning of your prompt. Now you are ready to install the python packages. \n",
    " `pip install tensorflow==2.0b matplotlib pandas nltk tensorflow-datasets` \n",
    " 7. Wait until all of the packages are done installing\n",
    "\n",
    "You have just completed the installation of Tensorflow 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster training, go to https://colab.research.google.com/notebook#create=true&language=python3 to create a new notebook and upload this notebook onto colab from the top left menu and clicking the file dropdown. Once the notebook is uploaded, click runtime, then change runtime type, and then click the hardware acceleration dropdown and select GPU. Then click save. This enables the use of a powerful Nvidia Tesla K80 for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Deep Learning\n",
    "![AI Image](https://miro.medium.com/max/945/0*jR0Oh-4sWsLCmrZQ.png)\n",
    "\n",
    "ML (Machine Learning) is a learning system where data is utilized to learn patterns make out meaningful results, and predict from multiple applications. DL (Deep Learning) /NN (Neural Networks) are specialized techniques to achieve ML goal more efficiently.\n",
    "\n",
    "The field of ML- especially DL has been growing rapidly in the past several years. Over the years, the conventional amount of enterprise data have increased from about 10,000–100,000 to 1,000,000–1,000,000,000. This caused a unique issue that researchers hadn’t encountered before; the limitations of compute power to train models. To create deeper neural networks, they needed a level of computational power that the time just couldn’t provide. This is why the field of ML plateaued for years. Then the rise of computational power allowed researchers to implement deeper, higher dimensional neural networks. New technological advances brought Graphical Processing Units (GPUs), one of the most efficient hardware for the high dimensional matrix multiplication required for deep learning. Again, the only limitation became the algorithms and architectures of the models, while we could constantly collect more data to improve accuracy.\n",
    "\n",
    "![Data to Accuracy Graph](https://miro.medium.com/max/861/0*X1xUgydehrTueSHZ.jpg)\n",
    "\n",
    "Now, the field of DL has split into multiple major groups: Computer Vision, Audio understanding and NLP to name some. There are a few major industries that NLP can completely redesign. These are customer service, virtual assistants, information retrieval.\n",
    "\n",
    "In customer assistance, chatbots can streamline customer service, taking care of simple tasks and questions and leaving complex queries to their human counterparts. In the future, DL models could analyze a call and rate the customer satisfaction through sentiment analysis. Virtual assistants use natural language understanding techniques to extract commands from your speech. Information retrieval extract valuable information from unstructured text by using sentiment analysis, and abstractive summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Basics\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*u8BChyGIqe6x-0x1)\n",
    "\n",
    "_Image Courtesy of MPLS VPN (_[_http://www.mplsvpn.info/2017/11/what-is-neuron-and-artificial-neuron-in.html_](http://www.mplsvpn.info/2017/11/what-is-neuron-and-artificial-neuron-in.html)_)_\n",
    "\n",
    "Deep Learning is modeled after the human neuron, where the neuron has dendrites that take in information from other neurons, the axon that does a computation, and axon terminals that send the signal to other neurons. In an artificial neuron, the input is synonymous to the dendrite, the activation function is synonymous to the axon and the output is like to axon terminal.\n",
    "\n",
    "A simple application of deep learning is to draw boundaries and insight from data.\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1011/0*ZjbGx97vgXmucjOA.png)\n",
    "\n",
    "Courtesy of  [StackoverFlow](https://stackoverflow.com/questions/12546583/how-do-i-draw-split-conditions-of-classification-tree-in-a-scatter-plot-of-r)\n",
    "\n",
    "In the example above, let's say the x axis is number of bedrooms in a house, and the y axis is the number of bathrooms in a house. A red dot represents an apartment, a green dot represents a single family house, and a blue dot represents a mansion. A common task in DL is to classify a house as an apartment, single family home, or a mansion given the number of bedrooms and the number of bathrooms it has. It does this by learning a function that draws a general boundaries around the 3 different classes.\n",
    "\n",
    "It is relatively easy for a human to draw these boundaries by hand by just looking at the plot. An example of a boundary that could be drawn is as follows:\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1011/1*69W22f2K06uZbbOa_tFh-w.png)\n",
    "\n",
    "While it is easy for us humans to make these boundaries when there are only 2 parameters, it gets increasingly more challenging as the number of parameters goes up. For example, if given four or five different aspects of the house, it would be near impossible to even visualize the data in our heads. This is the prime application of deep learning. For a computer, learning boundaries is quite rudimentary for problems with 10 or even 100 different features.\n",
    "\n",
    "If a house pops up with 1.5 bathrooms and 3 bedrooms, we can generalize our boundaries and predict that it is probably a single family home based on the data above. Just like this, a deep learning model can easily generalize for many data points, which means that it does not need to be specifically programmed for each data point and case scenario, and can use previous insight gained from data to decide on what decision to make.\n",
    "\n",
    "Lets see how exactly this is done:\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/989/0*PTCUvnpAAdV9RvPz)\n",
    "\n",
    "_Image Courtesy of Jeffries (_[_https://hackernoon.com/@dan.jeffries_](https://hackernoon.com/@dan.jeffries)_)_\n",
    "\n",
    "The above image is an image of what exactly is going on inside an artificial neuron. One artificial neuron is basically a linear model, in the form of y = Wx + B, or y = mx + b, which should look more familiar. W and B are parameters of the neuron which can be changed to fit the data. Finally, one of the activation functions will be applied to y which produces the output of the single neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all of the nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract the dataset from https://www.kaggle.com/kazanova/sentiment140/downloads/sentiment140.zip/2. Rename the csv file as data.csv and move it into your current working directory. If you are on a mac or running this notebook on Google Colab, run the script below to download and unzip your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"./drive/My Drive/data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up some constant variables for the model. In Deep Learning speak, these are called hyperparameters. What each variable represents will be covered later in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 100000\n",
    "BATCH_SIZE = 256\n",
    "TEST_DATA = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "Now wew will load the data. To do this, the python pandas library will be used which will make it easy to load the csv file into a understandable DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_columns=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "data = pd.read_csv(\"./data.csv\", encoding='cp1252', names=csv_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Perhaps the most important part of building a Deep Learning System is to process your raw data into something the computer can easily understand. While the Twitter dataset is relatively clean and straightforward, there are still steps we need to take so that the data can be easily fed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].map(lambda x: re.sub(r\"[^A-Za-z0-9 ]+\", \"\", x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem With Words\n",
    "\n",
    "Words cannot be directly fed into a model, because a computer has no idea what to do with them. Language data must always be converted to numerical form before they can be used for Deep Learning. There are 3 levels of abstraction we can use here:\n",
    "1. The first method is to simple replace each character in the dataset with a unique number corresponding to the character, such as using 1-26 for the alphabets in the english language.\n",
    "2. The second method is to represent each individual word as a vector. Because each word has a meaning attached to it, we first encode the word using a unique number to represent it in the vocabulary, then look it up in a set of embeddings, which can be thought of as a dictionary from which the computer gets the meaning of the word as a list of numbers\n",
    "3. The last method is a method to represent full sentences. Since this is a more complex strategy, it will not be covered in this notebook, but for those who are interested, a google search with keywords \"smooth inverse frequency embeddings\" will provide more information\n",
    "\n",
    "We first create a tokenizer which splits the data into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the vocab list from which the dictionary of words will be collected. The counter object used to check the frequency of the words appearing in the dataset so that vague and underused words can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for sent in data[\"text\"]:\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a vocab list containing the n most common words from the dataset. This is where the MAX_VOCAB_SIZE hyperparameter is used. A typical vocabulary size should be around 200000 to 500000 depending on the amount of memory available, but for speed purposes a smaller size of 100000 will be used for now. You can increase this if you feel that some words are not being represented in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(map(lambda x: x[0], vocab.most_common(MAX_VOCAB_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the Tensorflow dataset object is created from the data. The tf.data API makes it easy to deal with large amounts of data, different data formats, and complicated transformations. It also allows for GPU accelerations for some manipulations of data. This will allow us to quickly load our data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[\"text\"], data[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the text encoder which encodes each word with a unique number. This will represent the keys in the embedding layer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply the encoder function and a function to normalize the labels on the dataset. The beauty of the tf.Data api is that these computations are not done all at once, as it would take hours of time. It only produces data when needed, so long wait times for data to be processed are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(encode_map_fn)\n",
    "\n",
    "dataset = dataset.map(lambda x, y: (x, y/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we batch and pad the input data so that all inputs are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=([50],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing data allows us to see how well the model is performing on data is has never seen before. Technically the test dataset is called the validation dataset and the real test dataset is used to test the final produce, however for simplicity, there is only training and testing data. The typical ratio between the training and testing data should be around 80:20 for small datasets of around 100000 examples but can reach 99:1 for datasets of upwards of 1 million samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.take(TEST_DATA).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = dataset.skip(TEST_DATA).shuffle(5000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see an example of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_text, sample_labels = next(iter(test_dataset))\n",
    "\n",
    "sample_text[0], sample_labels[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Deep Learning Layers\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*nrMGDW7HmzpcIB12)\n",
    "\n",
    "_Image Courtesy of Sbongo (_[_https://www.kaggle.com/sbongo/_](https://www.kaggle.com/sbongo/tensorflow-nn-drop-out-batch-norm-lb-0-515)_)_\n",
    "\n",
    "Just as the brain is made up of trillions and trillions of these neurons, Neural Networks(NNs) are made up of many neurons interconnected.\n",
    "\n",
    "Fully connected layers are the most basic architecture, made up of hundreds of interconnected artificial neurons. Such architectures have 2 attributes: the depth and the hidden layer size. The depth is the total number of layers in the model while the hidden layer size is the number of neurons in each layer.\n",
    "\n",
    "The downside of this architecture is that as the input layer’s dimension gets increasingly large, such as when inputting millions of pixel values for high resolution images, the fully connected or dense layers no longer function as well.\n",
    "\n",
    "This leads to the next fundamental architecture called Convolution.\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*9gUD58U4ux-JbC7n)\n",
    "\n",
    "_Image Courtesy of Tamang (_[_https://medium.com/@apiltamang_](https://medium.com/@apiltamang)_)_\n",
    "\n",
    "Convolutional Neural Networks or ConvNets are location based architectures originally designed to read and interpret image pixels. They work by using sliding filters that take all of the values in the filter, multiply them by a weight and add a bias, and apply an activation function upon the sum. Then all of the values obtained from that operation are added up. This continues as the filter traverses through the image by some stride.\n",
    "\n",
    "The benefit of a ConvNet is that it can learn to recognize features in an image, from vertical and horizontal edges all the way to large objects and animals. This has led to ConvNets having immense success in the computer vision realm of DL, and are used in Tesla’s autopilot system to detect cars, pedestrians and road features such as lane lines, signs, and traffic signals.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*NCKWnZTHTJ-Qtku4)\n",
    "\n",
    "_Image Courtesy of Colah (_[_https://colah.github.io_](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)_)_\n",
    "\n",
    "Recurrent Neural Networks, or RNNs are used primarily for language or anything that has long term dependencies across it’s sequences, such as a speech recording or audio file. These networks split the inputs into time steps, where the computation from each time step is fed to the next, giving the model the ability to remember inputs. Such networks were specially designed for language, as the prominent architectures of the time were ineffective. The computation of the time step was also altered from the tradition artificial neuron. RNN architects implemented forget and retain gates, which provided the model with a way to remember information in a weighted way, analogous the humans remembering the important parts of their life and discarding mundane occurrences. These systems are called Long Short Term Memory Cells or LSTMs.\n",
    "\n",
    "These are the most common architectures for DL models, and can be used with each other to create more complex networks for complicated tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "The next step is to create the model. First we define the input layer and the embedding layer. If you recall, the embedding layer was similar to the dictionary of words and meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input([50])\n",
    "X = tf.keras.layers.Embedding(len(word_list)+2, 64)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add some convolutional layers. We use filter sizes of 2 and 3 to capture phrases in the data. For example we would want the model to classify \"not good\" as negative. This is the purpose of adding these convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = tf.keras.layers.Conv1D(64, 2)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Concatenate(axis=-2)([bigrams, X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the core layer for this model. The Bidirectional RNN will capture all of the information from the sequence of words. We use LSTM cells to help the model remember previously seen data in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add dense layers to the model. These layers can be seen as finalizing the model's output and making the last tweaks. Then we add the output layer which is a number ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for units in [32, 16]:\n",
    "    X = tf.keras.layers.Dense(units, activation='relu')(X)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "\n",
    "Neural Networks need a measure of how \"wrong\" they are, and this is what loss functions are. The following equation is the loss function for Cross Entropy Loss, where y is the actual training label of the data and the y with the hat is the model prediction.\n",
    "\n",
    "![](https://miro.medium.com/proxy/1*zi1wKAAGGt1Bn6mqo2MSFw.png)\n",
    "\n",
    "Cross Entropy loss is the most popular loss used for classification problems due to its simplicity and it’s effectiveness. When the actual label is 1, the second half of the function disappears whereas in the case that the actual label is 0 the first half is dropped off. A intuition to develop for why this loss works is to notice that when both the prediction and the actual datapoint are both either 1 or 0, then one of the terms in the loss cancels out and the output is 0. If the prediction and the data point are 1 and 0 respectively, the loss is maximized.An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong_._\n",
    "\n",
    "The process of training a deep learning model is analogous to tweaking each of the parameters in the model so that they fit the data as accurately as possible. A simple example of this is changing the parameters W and b in the equation y=Wx + b to find the line of best fit for data on a scatter plot.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*S9k6e_61NPsuFksp.png)\n",
    "\n",
    "Image Courtesy of Stackoverflow (  [https://stackoverflow.com/questions/19068862/how-to-overplot-a-line-on-a-scatter-plot-in-python](https://stackoverflow.com/questions/19068862/how-to-overplot-a-line-on-a-scatter-plot-in-python))\n",
    "\n",
    "In reality there are mathematical methods for solving for the optimal parameters W and b that best fit the data, but this is not possible in DL due to the immense magnitude of parameters. This can be visualized by looking at the graph for the loss function with regards to the parameters W and b for the above data, and the graph of the loss function for a DL model.\n",
    "\n",
    "The graph of the linear model.\n",
    "![](https://miro.medium.com/max/948/0*fsH5t9GEJrQ5MKsj)\n",
    "\n",
    "The graph for a Deep Learning Model:\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*bx8JAX4qdktI2omJ.png)\n",
    "The graph on the top can be easily solved for the parameters W and b when the loss is at the minimum, but for a DL model, this is impossible. Due to this, an iterative approach is taken to train a DL model.\n",
    "\n",
    "An optimization algorithm does this by taking the slope, or the gradient of the function at a given point, and then subtracts the slope from the parameters to move the point downhill to what is hopefully a minimum.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*_HJ7s8tq9BuqpQQj)\n",
    "\n",
    "Simply speaking, the parameters of the DL model are tweaked until they output correct predictions, after which the parameters are locked in place for use.\n",
    "\n",
    "The mathematical representation of this step is as follows:\n",
    "\n",
    "![Image result for gradient descent equation](https://miro.medium.com/proxy/0*8yzvd7QZLn5T1XWg.jpg)\n",
    "\n",
    "Here, theta is the parameter and the second term in the expression can be seen as the slope of the loss function with respect to the parameter theta. The alpha symbol represents the learning rate of the step, and controls how large of a step the algorithm makes. This is important because if the step is too small the algorithm will run very slow but if its too big the algorithm will fluctuate rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we comput the model on a random vector of digits to test if the output is a single digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict([[[0 for i in range(50)]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line adds a callback, which allows us to save the model's weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\"./weights.{epoch:02d}.ckpt\",\n",
    "                                          save_weights_only=True,\n",
    "                                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=3, validation_data=test_dataset, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, load the latest checkpoint from the model directory and run predictions on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./weights.3.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function will run everything needed to run predictions for a single sentence. It does everything done on the dataset previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sentence):\n",
    "    sent_tokens = encoder.encode(sentence)\n",
    "    sent_tokens = tf.keras.preprocessing.sequence.pad_sequences([sent_tokens], maxlen=50, padding=\"post\", truncating=\"post\")\n",
    "    return model.predict([sent_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(\"This prediction should be good!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
